# Frontier LLMs

## OpenAI
- [GPT1](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf)
- [GPT2](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
- [GPT3](https://arxiv.org/abs/2005.14165)
- [Codex](https://arxiv.org/abs/2107.03374)
- [InstructGPT](https://arxiv.org/abs/2203.02155)
- [GPT4](https://arxiv.org/abs/2303.08774)
- [GPT3.5](https://openai.com/index/chatgpt/)
- [GPT4o](https://openai.com/index/hello-gpt-4o/)
- [GPT o1](https://openai.com/index/introducing-openai-o1-preview/)
- [GPT o3](https://openai.com/index/deliberative-alignment/)

## Anthropic
- [Claude 3](https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf)
- [Claude 3.5](https://www.latent.space/p/claude-sonnet)
  
## Meta
- [Llama 1](https://arxiv.org/abs/2302.13971)
- [Llama 2](https://arxiv.org/abs/2307.09288)
- [Llama 3](https://arxiv.org/abs/2407.21783)

## Google
- [Gemini](https://arxiv.org/abs/2312.11805)
- [Gemini 2.0 Flash](https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#gemini-2-0-flash)
- [Gemini 2.0 Thinking](https://ai.google.dev/gemini-api/docs/thinking-mode)

## Gemma
- [Gemma](https://arxiv.org/abs/2408.00118)

## Mistral AI
- [Mistral 7B](https://arxiv.org/abs/2310.06825)
- [Mixtral of Experts](https://arxiv.org/abs/2401.04088)
- [Pixtral](https://arxiv.org/abs/2410.07073)

## DeepSeek
- [DeepSeek V1](https://arxiv.org/abs/2401.02954)
- [DeepSeek V2](https://arxiv.org/abs/2405.04434)
- [DeepSeek V3](https://github.com/deepseek-ai/DeepSeek-V3)
- [DeepSeek R1](https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-70B)
- [DeepSeek Coder](https://arxiv.org/abs/2401.14196)
- [DeepSeek Math](https://arxiv.org/abs/2402.03300)
- [DeepSeek MoE](https://arxiv.org/abs/2401.06066)

## Apple
- [Apple Intelligence](https://arxiv.org/abs/2407.21075)

### Modern BERT
- [ModernBERT](https://buttondown.com/ainews/archive/ainews-modernbert-small-new-retrieverclassifier/)
- [ColBERT](https://www.answer.ai/posts/colbert-pooling.html)

### Other Notable Models
- AI2:
  - [Olmo](https://arxiv.org/abs/2402.00838)
  - [Molmo](https://arxiv.org/abs/2409.17146)
  - [OlmOE](https://arxiv.org/abs/2409.02060)
  - [TÃ¼lu 3](https://allenai.org/blog/tulu-3-technical)
  - [Olmo 2](https://x.com/soldni/status/1875266934943649808?s=46)
- [Grok](https://github.com/xai-org/grok-1)
- [Amazon Nova](https://buttondown.com/ainews/archive/ainews-olympus-has-dropped-aka-amazon-nova/)
- [Yi](https://www.wired.com/story/chinese-startup-01-ai-is-winning-the-open-source-ai-race/)
- [Reka](https://www.latent.space/p/yitay)
- [Jamba](https://buttondown.com/ainews/archive/ainews-jamba-mixture-of-architectures-dethrones/)
- [Cohere](https://cohere.com/command)
- [Nemotron](https://buttondown.com/ainews/archive/ainews-to-be-named-2748/)
- [Microsoft Phi](https://arxiv.org/abs/2412.08905)
- [HuggingFace SmolLM](https://www.latent.space/p/2024-open-models)

#### Scaling Laws
- [Kaplan](http://arxiv.org/abs/2001.08361)
- [Chinchilla](https://arxiv.org/abs/2203.15556)
- [Emergence](https://arxiv.org/abs/2206.07682)
- [Mirage](https://arxiv.org/abs/2304.15004)
- [Post-Chinchilla laws](https://arxiv.org/abs/2401.00448)

### 2025 Frontier Models
- OpenAI o1 and [o3](https://en.wikipedia.org/wiki/OpenAI_o3)
- [DeepSeek R1](https://api-docs.deepseek.com/news/news1120)
- [QwQ](https://qwenlm.github.io/blog/qwq-32b-preview/) and [QVQ](https://qwenlm.github.io/blog/qvq-72b-preview/)
- [Fireworks f1](https://fireworks.ai/blog/fireworks-compound-ai-system-f1)

### Reasoning Models Research
- [Let's Verify Step By Step](https://arxiv.org/abs/2305.20050)
- [STaR](https://www.youtube.com/watch?v=Y5-FeaFOEFM)
- [Noam Brown's Research](https://www.youtube.com/live/Gr_eYXdHFis)